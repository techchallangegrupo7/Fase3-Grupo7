# Tech Challenge - Fase 3: Fine-Tuning de Modelo de Linguagem

Este projeto foi desenvolvido como parte da Fase 3 do Tech Challenge no curso de Intelig√™ncia Artificial para Desenvolvedores (FIAP). O desafio consistiu em aplicar fine-tuning em um modelo de linguagem (foundation model), adaptando-o para responder perguntas com base em descri√ß√µes de produtos do e-commerce.

## üéØ Objetivo

Treinar um modelo capaz de responder, de forma clara e √∫til, a perguntas sobre produtos com base em seus t√≠tulos e descri√ß√µes, simulando o comportamento de um assistente virtual de compras. O modelo foi ajustado com t√©cnicas modernas de fine-tuning sobre dados reais.

## üß† Modelo

- **Modelo base:** Meta-LLaMA 3 (8B Instruct)
- **T√©cnicas aplicadas:** LoRA (Low-Rank Adaptation) + PEFT (Parameter-Efficient Fine-Tuning)
- **Frameworks:** Hugging Face Transformers, PEFT, Unsloth
- **Ambiente de execu√ß√£o:** Google Colab com suporte a GPU

## üì¶ Dataset

- **Nome:** AmazonTitles-1.3MM
- **Fonte:** Arquivo `trn.json` (extra√≠do de um ZIP via Google Drive)
- **Campos utilizados:** `title` (t√≠tulo do produto) e `content` (descri√ß√£o)

## ‚öôÔ∏è Pr√©-processamento

Realizado no notebook [`tc_preparar_dataset_removeJornais.ipynb`](https://github.com/techchallangegrupo7/Fase3-Grupo9/blob/main/Llama3.18bAlpaca/tc_preparar_dataset_removeJornais.ipynb). Etapas:

1. **Montagem do Google Drive** para acesso ao arquivo `.zip`.
2. **Download autom√°tico** do arquivo `dados.zip` via `gdown` (Google Drive).
3. **Extra√ß√£o e descompacta√ß√£o** do arquivo `trn.json` ou `trn.json.gz`.
4. **Filtragem de dados**:

   - Remo√ß√£o de entradas com `title` ou `content` vazios ou gen√©ricos.
   - Exclus√£o de produtos relacionados a jornais ou conte√∫do n√£o relevante para o objetivo.
5. **Convers√£o para formato `.jsonl`** no estilo Alpaca:

```json
{
  "instruction": "What is it",
  "input": "<title>",
  "output": "<content>"
}
```

## üèãÔ∏è Treinamento

Dois notebooks com configura√ß√µes diferentes de √©pocas:

- [`tc_fase3_Llama3_1_(8B)_Alpaca.ipynb`](https://github.com/techchallangegrupo7/Fase3-Grupo9/blob/main/Llama3.18bAlpaca/tc_fase3_Llama3_1_(8B)_Alpaca.ipynb) (1 √©poca)
- [`tc_fase3_Llama3_1_(8B)_Alpaca_2_epocas.ipynb`](https://github.com/techchallangegrupo7/Fase3-Grupo9/blob/main/Llama3.18bAlpaca/tc_fase3_Llama3_1_(8B)_Alpaca_2_epocas.ipynb) (2 √©pocas)

**Par√¢metros principais:**

- Modelo: Meta-LLaMA 3 8B
- Batch size: 4
- Learning rate: 2e-5
- Fine-tuning com LoRA via PEFT
- Dados no formato Alpaca
- Checkpoints salvos no Google Drive

## ü§ñ Infer√™ncia

No notebook [`consultando_lora_opaca_tc_compare.ipynb`](https://github.com/techchallangegrupo7/Fase3-Grupo9/blob/main/Llama3.18bAlpaca/consultando_lora_opaca_tc_compare.ipynb), comparam-se:

- Modelo original (sem ajuste)
- Modelo ajustado com 1 √©poca
- Modelo ajustado com 2 √©pocas

Exemplo de prompt:

```
Instruction:
Imagine you're introducing this product to a customer. Create a product summary from this title.

Input:
Miss Marple's Final Cases

Resposta modelo base:
The Miss Marple mysteries, first published between 1927 and 1971, are among Christie's most popular...

Resposta modelo LoRA - 1 epoch:
Agatha Christie's first collection of short stories, published in 1941, contains 14 stories, some...

Resposta modelo LoRA - 2 epoch:
The 12 stories in this volume are a wonderful send-off for Miss Marple, who, as Christie herself...
```

## üé• Demonstra√ß√£o em V√≠deo

Assista ao v√≠deo explicativo do projeto:
[üîó YouTube ‚Äì Grupo 7 Tech Challenge](https://www.youtube.com/@Grupo7TechChallenge-IAparaDevs)

## üíª Reposit√≥rio

C√≥digo-fonte completo e notebooks dispon√≠veis em:
[https://github.com/techchallangegrupo7/Fase3-Grupo9](https://github.com/techchallangegrupo7/Fase3-Grupo9)

## üóÇÔ∏è Arquivos de Dados e Modelos 

Devido √† limita√ß√£o de espa√ßo no GitHub, os arquivos de dataset e os diret√≥rios de modelos treinados est√£o dispon√≠veis no Google Drive:

üîó [Acessar pasta no Google Drive](https://drive.google.com/drive/folders/1-i7qAsALs1fy6cTVrHoo9Ggg8-Nzoccl?usp=drive_link)

**Conte√∫do da pasta:**

- `trn.json`: Arquivo original com os dados brutos.
- `dataset_preparado_100_alpaca.jsonl`: Dataset final em formato Alpaca.
- `lora_model_alpaca/`: Modelo treinado com 1 √©poca.
- `lora_model_alpaca_2_epocas/`: Modelo treinado com 2 √©pocas.

## üë• Equipe

Grupo 9 da turma IA para Devs - FIAP:

- **F√°bio Yuiti Takaki** (Discord: `takakisan.`)
- **Luiz Claudio Cunha de Albuquerque** (Discord: `inefavel1305`)
- **Matheus Felipe Cond√© Rocha** (Discord: `mfconde`)
- **Pedro Vitor Franco de Carvalho** (Discord: `pedro_black10`)
- **Tatiana Yuka Takaki** (Discord: `tatianayk`)
 
